{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental NP chunking\n",
    "========================\n",
    "\n",
    "Estnltk includes an experimental noun phrase chunker, which can be used\n",
    "to detect non-overlapping noun phrases from the text.\n",
    "\n",
    "Basic usage\n",
    "-----------\n",
    "\n",
    "The class **NounPhraseChunker** provides method\n",
    "**analyze\\_text()**, which takes a\n",
    "**Text** object as an input, detects potential noun\n",
    "phrases, and stores in the layer `NOUN_CHUNKS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NOUN_CHUNKS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1533f62602b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mestnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mestnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_chunker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNounPhraseChunker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mestnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOUN_CHUNKS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NOUN_CHUNKS'"
     ]
    }
   ],
   "source": [
    "# TODO: check if works on devel \n",
    "\n",
    "from estnltk import Text\n",
    "from estnltk.np_chunker import NounPhraseChunker\n",
    "from estnltk.names import TEXT, NOUN_CHUNKS\n",
    "from pprint import pprint\n",
    "\n",
    "# initialise the chunker\n",
    "chunker = NounPhraseChunker()\n",
    "\n",
    "text = Text('Suur karvane kass nurrus punasel diivanil, väike hiir aga hiilis temast mööda.')\n",
    "# chunk the input text\n",
    "text = chunker.analyze_text( text )\n",
    "\n",
    "# output the results (found phrases)\n",
    "pprint( text[NOUN_CHUNKS] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the method\n",
    ":py\\~estnltk.np\\_chunker.NounPhraseChunker.analyze\\_text returns the\n",
    "input text. The keyword argument `return_type` can be used to change the\n",
    "type of data returned. If `return_type='labels'`, the method returns\n",
    "results of chunking in a BIO annotation scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "(' Unexpected return type: ', 'labels')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-428b19369789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# chunk the input text, get the results in BIO annotation format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnp_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_text\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'labels'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# output results of the chunking in BIO annotation format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dage/anaconda3/envs/working_estnltk/lib/python3.5/site-packages/estnltk/np_chunker.py\u001b[0m in \u001b[0;36manalyze_text\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0mreturn_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margVal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' Unexpected return type: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' Unsupported argument given: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: (' Unexpected return type: ', 'labels')"
     ]
    }
   ],
   "source": [
    "# TODO: check if works on devel \n",
    "\n",
    "from estnltk import Text\n",
    "from estnltk.np_chunker import NounPhraseChunker\n",
    "from estnltk.names import TEXT\n",
    "\n",
    "# initialise the chunker\n",
    "chunker = NounPhraseChunker()\n",
    "\n",
    "text = Text('Suur karvane kass nurrus punasel diivanil, väike hiir aga hiilis temast mööda.')\n",
    "\n",
    "# chunk the input text, get the results in BIO annotation format\n",
    "np_labels = chunker.analyze_text( text, return_type='labels' )\n",
    "\n",
    "# output results of the chunking in BIO annotation format\n",
    "for word, np_label in zip(text.words, np_labels):\n",
    "    print( word[TEXT]+'  '+str(np_label) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the resulting list `np_labels` contains a label\n",
    "for each word in the input text, indicating word's position in phrase:\n",
    "`\"B\"` denotes that the word begins a phrase, `\"I\"` indicates that the\n",
    "word is inside a phrase, and `\"O\"` indicates that the word does not\n",
    "belong to any noun phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input argument `return_type=\"strings\"` is passed to the method,\n",
    "the method returns only results of the chunking as a list of phrase\n",
    "strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from estnltk import Text\n",
    "from estnltk.np_chunker import NounPhraseChunker\n",
    "\n",
    "# initialise the chunker\n",
    "chunker = NounPhraseChunker()\n",
    "\n",
    "text = Text('Autojuhi lapitekk pälvis linna koduleheküljel paljude kodanike tähelepanu.')\n",
    "# chunk the input text\n",
    "phrase_strings = chunker.analyze_text( text, return_type=\"strings\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example produces following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Autojuhi lapitekk', 'linna koduleheküljel', 'paljude kodanike tähelepanu']\n"
     ]
    }
   ],
   "source": [
    "print( phrase_strings )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `return_type=\"tokens\"` is set, the chunker returns a list of lists of\n",
    "tokens, where each token is given as a dictonary containing analyses of\n",
    "the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autojuhi autojuht\n",
      "lapitekk lapitekk\n",
      "\n",
      "linna linn\n",
      "koduleheküljel kodulehekülg\n",
      "\n",
      "paljude palju\n",
      "kodanike kodanik\n",
      "tähelepanu tähelepanu\n"
     ]
    }
   ],
   "source": [
    "from estnltk import Text\n",
    "from estnltk.np_chunker import NounPhraseChunker\n",
    "from estnltk.names import TEXT, ANALYSIS, LEMMA\n",
    "\n",
    "# initialise the chunker\n",
    "chunker = NounPhraseChunker()\n",
    "\n",
    "text = Text('Autojuhi lapitekk pälvis linna koduleheküljel paljude kodanike tähelepanu.')\n",
    "# chunk the input text\n",
    "phrases = chunker.analyze_text( text, return_type=\"tokens\" )\n",
    "# output phrases word by word\n",
    "for phrase in phrases:\n",
    "    print()\n",
    "    for token in phrase:\n",
    "        # output text and first lemma\n",
    "        print( token[TEXT], token[ANALYSIS][0][LEMMA] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, regardless the `return_type`, the layer `NOUN_CHUNKS` will\n",
    "always be added to the input Text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutting phrases\n",
    "---------------\n",
    "\n",
    "By default, the chunker does not allow tagging phrases longer than 3\n",
    "words, as the quality of tagging longer phrases is likely suboptimal,\n",
    "and the coverage of these phrases is also likely low [[1]](#ref) . So, phrases\n",
    "longer than 3 words will be cut into one word phrases. This default\n",
    "setting can be turned off by specifying `cutPhrases=False` as an input\n",
    "argument for the method **analyze\\_text()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from estnltk import Text\n",
    "from estnltk.np_chunker import NounPhraseChunker\n",
    "\n",
    "# initialise the chunker\n",
    "chunker = NounPhraseChunker()\n",
    "\n",
    "text = Text('Kõige väiksemate tassidega serviis toodi kusagilt vanast tolmusest kapist välja.')\n",
    "\n",
    "# chunk the input text while allowing phrases longer than 3 words\n",
    "phrase_strings = chunker.analyze_text( text, cutPhrases=False, return_type=\"strings\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kõige väiksemate tassidega serviis', 'vanast tolmusest kapist']\n"
     ]
    }
   ],
   "source": [
    "print( phrase_strings )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a custom syntactic parser\n",
    "-------------------------------\n",
    "\n",
    "By default, the MaltParser is used for obtaining the syntactic\n",
    "annotation, which is used as a basis in the chunking. Using the keyword\n",
    "argument `parser` in the initialization of the\n",
    "**NounPhraseChunker**, you can specify a custom\n",
    "parser to be used during the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'estnltk.syntax.parsers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3a518af08afb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mestnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mestnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_chunker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNounPhraseChunker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mestnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyntax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVISLCG3Parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# initialise the chunker using VISLCG3Parser instead of MaltParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'estnltk.syntax.parsers'"
     ]
    }
   ],
   "source": [
    "# TODO: check if works on devel\n",
    "\n",
    "from estnltk import Text\n",
    "from estnltk.np_chunker import NounPhraseChunker\n",
    "from estnltk.syntax.parsers import VISLCG3Parser\n",
    "\n",
    "# initialise the chunker using VISLCG3Parser instead of MaltParser\n",
    "chunker = NounPhraseChunker( parser = VISLCG3Parser() )\n",
    "\n",
    "text = Text('Maril oli väike tall.')\n",
    "# chunk the input text\n",
    "text = chunker.analyze_text( text )\n",
    "\n",
    "# output the results (found phrases)\n",
    "pprint( text[NOUN_CHUNKS] )\n",
    "\n",
    "[{'end': 5, 'start': 0, 'text': 'Maril'},\n",
    " {'end': 20, 'start': 10, 'text': 'väike tall'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "**References**\n",
    "\n",
    "[1]: An automatic analysis of the Balanced Corpus of Estonian suggests\n",
    "    that only 3% of all NP chunks are longer than 3 words."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
