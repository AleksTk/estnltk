{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Text objects from large corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EstNLTK contains modules for importing texts from the [Estonian Reference Corpus](http://www.cl.ut.ee/korpused/segakorpus/) (_Eesti keele koondkorpus_), the [Estonian Web 2013 corpus](https://metashare.ut.ee/repository/browse/ettenten-korpus-toortekst/b564ca760de111e6a6e4005056b4002419cacec839ad4b7a93c3f7c45a97c55f) (aka _etTenTen 2013_), and [the Estonian National Corpus 2017](https://metashare.ut.ee/repository/browse/estonian-national-corpus-2017/b616ceda30ce11e8a6e4005056b40024880158b577154c01bd3d3fcfc9b762b3/) (_Eesti keele ühendkorpus 2017_). \n",
    "Modules allow to import corpus content as EstNLTK Text objects, and also to preserve original annotations (e.g. paragraph and sentence boundary annotations) in the corpus (to an extent).\n",
    "In sections below, we will introduce these modules in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Importing texts of the Estonian Reference Corpus (_Eesti keele koondkorpus_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the files\n",
    "\n",
    "The page [http://www.cl.ut.ee/korpused/segakorpus/](http://www.cl.ut.ee/korpused/segakorpus/) lists all the subcorpora of Estonian Reference Corpus. From there, you can follow the links and download (zipped) XML files of subcorpora. \n",
    "\n",
    "Once you have downloaded a zipped XML corpus and unzipped it, you should see a folder structure similar to this:\n",
    "\n",
    "\n",
    "        ├── Kroonika\n",
    "        │   ├── bin\n",
    "        │   │   ├── koondkorpus_main_header.xml\n",
    "        │   │   └── tei_corpus.rng\n",
    "        │   └── Kroon\n",
    "        │       ├── bin\n",
    "        │       │   └── header_aja_kroonika.xml\n",
    "        │       └── kroonika\n",
    "        │           ├── kroonika_2000\n",
    "        │           │   ├── aja_kr_2000_12_08.xml\n",
    "        │           │   ├── aja_kr_2000_12_15.xml\n",
    "        │           │   ├── aja_kr_2000_12_22.xml\n",
    "        │           │   └── aja_kr_2000_12_29.xml\n",
    "        │           ├── kroonika_2001\n",
    "        │           │   ├── aja_kr_2001_01_05.xml\n",
    "        │           │   ├── aja_kr_2001_01_12.xml\n",
    "        │           │   ├── aja_kr_2001_01_19.xml\n",
    "        │           │   ├── aja_kr_2001_01_22.xml\n",
    "        ...         ...     ...\n",
    "        \n",
    "        \n",
    "<center>Example. Folder structure in _Kroonika.zip_</center>\n",
    "\n",
    "Folders `'bin'` contain headers and corpus descriptions. The `'.xml'` files outside the `'bin'` folders are the files with the actual textual content. These files can be loaded with EstNLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Texts from a single XML file\n",
    "\n",
    "You can use function **`parse_tei_corpus`** to create Text objects based on a single XML TEI file. \n",
    "An example:\n",
    "\n",
    "    from estnltk.corpus_processing.parse_koondkorpus import get_div_target\n",
    "    from estnltk.corpus_processing.parse_koondkorpus import parse_tei_corpus\n",
    "    \n",
    "    # input file (must be with a full path)\n",
    "    input_xml_file = \"C:\\Kroonika\\Kroon\\kroonika\\kroonika_2000\\aja_kr_2000_12_08.xml\"\n",
    "    \n",
    "    # find out which subsection of the XML file forms a single document\n",
    "    target = get_div_target( input_xml_file )   \n",
    "    \n",
    "    # import documents as Text objects   \n",
    "    for text_obj in parse_tei_corpus( input_xml_file, target=[target] ):\n",
    "        # TODO: do something with the Text object\n",
    "        ...\n",
    "\n",
    "Note that before using the function, you should decide, which subsections of the XML file you want to consider as single document (the argument `target`). If you do not have clear preferences, you can use the function `get_div_target` to get a reasonable default for every subcorpus of the Reference Corpus.\n",
    "\n",
    "**_Layers._** By default, obtained Text objects do not have any annotation layers. See the section \"Details of the `parse_koondkorpus` module\" on how to add layers.\n",
    "\n",
    "**_Metadata._** Obtained Text objects also have metadata, stored in the dictionary `text_obj.meta`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Texts from a directory of XML files\n",
    "\n",
    "You can use function **`parse_tei_corpora`** to create Text objects from all (or selected) XML TEI files in a directory and in all of its subdirectories. \n",
    "An example:\n",
    "\n",
    "    from estnltk.corpus_processing.parse_koondkorpus import parse_tei_corpora\n",
    "    \n",
    "    # input directory (must be with a full path)\n",
    "    input_xml_path = 'C:\\\\Kroonika\\\\Kroon\\\\kroonika'\n",
    "   \n",
    "    # import documents as Text objects   \n",
    "    for text_obj in parse_tei_corpora( input_xml_path, target=['artikkel'] ):\n",
    "        # TODO: do something with the Text object\n",
    "        ...\n",
    "\n",
    "Note that function `parse_tei_corpora` will traverse recursively all subdirectories of the given directory (`input_xml_path`), and will also extract Text objects from XML TEI files in the subdirectories. In addition, you can use arguments `prefix` and `suffix` to further specify which file names are suitable for extraction, e.g. `prefix=\"aja_kr_2000_\"` will tell the function to extract Texts only from files starting with the string `'aja_kr_2000_'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of the `parse_koondkorpus` module\n",
    "\n",
    "#### Reconstruction of source text\n",
    "\n",
    "As the original plain texts for _koondkorpus_ XML files are not available, functions `parse_tei_corpus` and `parse_tei_corpora` will reconstruct the source texts by themselves. \n",
    "By default, this reconstruction follows the original XML mark-up: paragraphs (texts between`<p>` and `</p>` tags in the original XML) will be separated by double newlines, sentences (texts between `<s>` and `</s>` tags in the XML) will be separated by newlines, and words will be separated by whitespaces in the constructed texts.\n",
    "Optionally, you may also add tokenization layers to created `Text` objects: either by strictly following the original XML tokenization mark-up, or by adding tokenization with EstNLTK's tools.\n",
    "\n",
    "#### Signatures of the parsing functions\n",
    "\n",
    "  * **`parse_tei_corpus`**`(path, target=['artikkel'], encoding='utf-8', add_tokenization=False, preserve_tokenization=False, record_xml_filename=False, sentence_separator='\\n', paragraph_separator='\\n\\n', orig_tokenization_layer_name_prefix='')` -- reads and parses a single XML file (given with the full `path`), creates `Text` objects storing documents and metadata from the file, and returns a list of created `Text` objects;\n",
    "\n",
    "\n",
    "  * **`parse_tei_corpora`**`(root, prefix='', suffix='.xml', target=['artikkel'], encoding='utf-8', add_tokenization=False, preserve_tokenization=False, record_xml_filename=False, sentence_separator='\\n', paragraph_separator='\\n\\n', orig_tokenization_layer_name_prefix='')` -- reads recursively all the files from the directory `root`, selects files with the given `prefix` and `suffix` for XML parsing, and creates `Text` objects storing documents and metadata from the files. Returns a list of created `Text` objects;\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "Exact behaviour of the functions can be modified by the following common arguments:\n",
    "\n",
    "* **`target`** -- specifies the list of types of divs, from which the textual content is to be extacted. For instance, in case of newspaper articles, the content of an article is typically between `<div3 type=\"artikkel\">` and `</div3>`, so, you should use `target=['artikkel']`. In case of fiction writings, the content of a single work is typically between `<div1 type=\"tervikteos\">` and `</div1>`, and you may want to use `target=['tervikteos']`.\n",
    "\n",
    "  Which values should be used for `target` depend on the subcorpus and the goal of analysis. For example, you may want to divide a fiction text into chapters, instead of analysing it as a whole. In such case, you should manually look up the correct type values (for chapters) from the concrete XML file.\n",
    "  \n",
    "  If you do not have very specific goals, you can use the function **`get_div_target()`**, which provides a reasonable default div type for the given XML file, based on the hard-coded values. Example:\n",
    "\n",
    "       from estnltk.corpus_processing.parse_koondkorpus import get_div_target\n",
    "       from estnltk.corpus_processing.parse_koondkorpus import parse_tei_corpus\n",
    "       \n",
    "       xml_file = 'C:\\\\Eesti_ilukirjandus\\\\ilukirjandus\\\\Eesti_ilukirjandus_1990\\\\'+\\\n",
    "                  'ilu_ahasveerus.tasak.xml'\n",
    "       target = get_div_target( xml_file )    # note: returns a single value, not list\n",
    "       docs = parse_tei_corpus( xml_file, target=[target] )\n",
    "  \n",
    "     Note: the function `get_div_target()` needs name of the xml file with full path, as it uses information from directory names for determining the div type.\n",
    "\n",
    "\n",
    "* **`add_tokenization`** -- specifies whether the tokenization layers (`'tokens'`, `'compound_tokens'`, `'words'`, `'sentences'`, and `'paragraphs'`) should be added to newly created Text objects (default: False). Note that if `preserve_tokenization==False`, then the tokenization layers are added with EstNLTK's tools, otherwise the original layers from the XML mark-up are preserved;\n",
    "\n",
    "\n",
    "* **`preserve_tokenization`** -- specifies if the original word, sentence and paragraph tokenization from the XML mark-up should be preserved (default: False). This only works if `add_tokenization` is switched on. Then `Text` objects are created with layers `'tokens'`, `'compound_tokens'`, `'words'`, `'sentences'` and `'paragraphs'`, which preserve the original tokenization. This means that paragraphs are taken from between `<p>` and `</p>` tags, sentences from between `<s>` and `</s>` tags, and words are taken as space-separated tokens inside the sentences. The layer `'compound_tokens'` will always remain empty (because there is no information about token compounding in the XML mark-up), and the layer `'tokens'` will be equal to the layer `'words'`;\n",
    "\n",
    "     _Note_: Creating tokenization layers typically takes more processing time than loading `Text` objects without layers. If you do not change parameters `sentence_separator` and `paragraph_separator`, then reconstructed texts also preserve the hints about tokenization, and you can restore the original tokenization afterwards. See the remark **C** below for details;\n",
    "     \n",
    "\n",
    "* **`record_xml_filename`** -- specifies if the name of XML file should recorded in the metadata of the created `Text` objects, under the key `'_xml_file'` (default: False);\n",
    "\n",
    "\n",
    "* **`encoding`** -- encoding of the input file (or input files). Normally, you should go with the default value ('utf-8').\n",
    "\n",
    "\n",
    "* **`sentence_separator`** -- string used as sentence separator during the reconstruction of the text (default: `'\\n'`).\n",
    "\n",
    "\n",
    "* **`paragraph_separator`** -- string used as paragraph separator during the reconstruction of the text (default: `'\\n\\n'`).\n",
    "\n",
    "\n",
    "* **`orig_tokenization_layer_name_prefix`** -- string used as a prefix in names of layers of original tokenization. You can use this argument to make names of original tokenization layers distinguishable from EstNLTK's tokenization layers. (default: `''`)\n",
    "\n",
    "\n",
    "\n",
    "#### What to keep in mind when using the loading functions\n",
    "\n",
    "   * **A.** Functions `parse_tei_corpus` and `parse_tei_corpora` provide a simple and general solution to loading texts from XML TEI failes. The textual content (and also metadata, up to an extent) can be loaded from any subcorpus of the Estonian Reference Corpus. However, the genericity has a cost: typically, corpus-specific markings will not be loaded. For instance, the corpus of Estonian parliamentary transcripts also contains special markings for speaker names, but this information is not loaded. And the corpus of Internet forums also records time and user name for each forum message, but only the message itself (the textual content) is loaded.\n",
    " \n",
    "   If you need to \"get more out of\" the XML TEI files, you'll need to create your own loading functions. You can follow the example of the functions in the module `estnltk.corpus_processing.parse_koondkorpus`, and use the library [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to do the job.\n",
    "\n",
    "\n",
    "   * **B.** _Tokenization_: adding EstNLTK's tokenization vs preserving the original tokenization. \n",
    "   \n",
    "      1. If you use EstNLTK's tokenization instead of the original one, you get annotations of multiword tokens (`'compound_tokens'`), and this also helps to get more accurate sentence annotations. For instance, the original markup occasionally contains sentence endings inside multiword names, such as _'Ju. M. Lotman'_, and date expressions, such as _'24 . 10. 1921'_, but EstNLTK's default tokenization marks such expressions as `'compound_tokens'` and thus cancels sentence endings inside these expressions. If you use EstNLTK's tokenization, it is advisable also to set `sentence_separator=' '`, so that reconstructed text will not contain newlines inside compound tokens;\n",
    "    \n",
    "      2. If you preserve the original tokenization, you can get better alignment with other linguistic annotations laid on the loaded corpus, such as [Dependency Treebank annotations](https://github.com/EstSyntax/EDT), or [TimeML annotations](https://github.com/soras/EstTimeMLCorpus). Data also gets loaded faster with the original tokenization annotations. The downsides are: there will be no `'compound_tokens'` marked in loaded texts, and there may be more sentence tokenization errors;\n",
    "   \n",
    "   \n",
    "   * **C.** _Loading texts without tokenization, and then restoring the original tokenization later._ If you use parameters `sentence_separator` and `paragraph_separator` with their default values, you can restore the original tokenization after loading the texts. For this:\n",
    "   \n",
    "      1. Use a `CompoundTokenTagger` with initialization parameter `do_not_join_on_strings=['\\n', '\\n\\n']` to ensure that compound tokens will not cross out sentence and paragraph endings;\n",
    "      2. Use a  `SentenceTokenizer` with the `base_sentence_tokenizer` set to NLTK's `LineTokenizer()`, so that texts will be split to sentences following newlines;\n",
    "      3. Use the default paragraph tokenizer to create paragraphs according to the original paragraph tokenization;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing whole Estonian Reference Corpus with EstNLTK\n",
    "\n",
    "If you need to process the whole Estonian Reference Corpus with EstNLTK, you can use the command-line scripts in [ **`https://github.com/estnltk/estnltk-workflows`**](https://github.com/estnltk/estnltk-workflows). \n",
    "The workflow for processing Estonian Reference Corpus with EstNLTK, and saving the results as JSON format files is located at: \n",
    "[https://github.com/estnltk/estnltk-workflows/tree/master/estnltk_workflows/koondkorpus_and_ettenten_to_json](https://github.com/estnltk/estnltk-workflows/tree/master/estnltk_workflows/koondkorpus_and_ettenten_to_json). \n",
    "For details about processing, check out the `readme.md` file in the folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Importing texts of the Estonian Web 2013 corpus (_etTenTen 2013 korpus_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the corpus\n",
    "\n",
    "You can download the etTenTen 2013 corpus from here [https://metashare.ut.ee/repository/browse/ettenten-korpus-toortekst/b564ca760de111e6a6e4005056b4002419cacec839ad4b7a93c3f7c45a97c55f](https://metashare.ut.ee/repository/browse/ettenten-korpus-toortekst/b564ca760de111e6a6e4005056b4002419cacec839ad4b7a93c3f7c45a97c55f). After unpacking the content, you should get one large file with the extension `vert` or `prevert` (e.g. `ettenten13.processed.prevert`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Texts\n",
    "\n",
    "You can use function **`parse_ettenten_corpus_file_iterator`** to iterate over the corpus file and get Text objects yielded one-by-one.\n",
    "An example:\n",
    "\n",
    "    from estnltk.corpus_processing.parse_ettenten import parse_ettenten_corpus_file_iterator\n",
    "    \n",
    "    # input file\n",
    "    input_file = \"ettenten13.processed.prevert\"\n",
    "   \n",
    "    # iterate over extracted Text objects   \n",
    "    for text_obj in parse_ettenten_corpus_file_iterator( input_file ):\n",
    "        # TODO: do something with the Text object\n",
    "        ...\n",
    "\n",
    "**_Layers._** By default, created Text objects have only one annotation layer: `'original_paragraphs'`. The layer contains original paragraph markings (between `<p>` and `</p>` tags) from the input file. Unlike estnltk's `'paragraphs'` layer, which envelops around `'sentences'` layer, `'original_paragraphs'` will be a stand-alone layer of Text object (because sentences are not annotated in the input file).\n",
    "\n",
    "**_Metadata._** Obtained Text objects also have metadata, stored in the dictionary `text_obj.meta`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of the `parse_ettenten` module\n",
    "\n",
    "#### Reconstruction of source text\n",
    "\n",
    "The etTenTen 2013 corpus contains documents crawled from the web. \n",
    "Documents have been cleaned up from most of the HTML (and other XML) tags, although few tags remain here and there.\n",
    "Textual content of the web pages has been preserved along with its segmentation into paragraphs.\n",
    "\n",
    "The function `parse_ettenten_corpus_file_iterator` collects documents and creates Text objects in a straightforward manner: all paragraphs (texts between`<p>` and `</p>` tags in the original file) will be concatenated by a double newline to reconstruct the text. If there are any other XML/HTML tags between `<p>` and `</p>` tags, these tags will also be included in the textual content.\n",
    "\n",
    "#### Signature of the parsing function\n",
    "\n",
    "  * **`parse_ettenten_corpus_file_iterator`**`(in_file, encoding='utf-8', focus_doc_ids=None, add_tokenization=False,        discard_empty_paragraphs=True, store_paragraph_attributes=False, paragraph_separator='\\n\\n')` -- reads and parses etTenTen corpus file (`in_file`), and on the progress, creates `Text` objects storing documents and metadata, and yields created `Text` objects one-by-one;\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "Exact behaviour of the function can be modified by the following arguments:\n",
    "\n",
    "* **`in_file`** -- full name of etTenTen corpus file.\n",
    "\n",
    "\n",
    "* **`encoding`** -- encoding of the input file. Normally, you should go with the default value ('utf-8').\n",
    "\n",
    "\n",
    "* **`focus_doc_ids`** -- set of document id-s corresponding to the documents which need to be extracted from the `in_file`. **Important:** this should be a set of strings, not a set of integers. If provided, then only documents with given id-s will be       processed, and all other documents will be skipped. If the value is `None` or empty set, then all documents in the file will be processed. (default: `None`).\n",
    "\n",
    "\n",
    "* **`add_tokenization`** -- Specifies if (full) tokenization will be added to reconstructed texts (default: False). If `add_tokenization==False`, then there will be only one tokenization layer: `'original_paragraphs'`, which will be collected from XML annotations and added as a stand-alone annotation layer of a Text object. If `add_tokenization==True`, then there will be layers `'tokens'`, `'compound_tokens'`, `'words'`, and `'sentences'` (created by EstNLTK's default tokenizers) and layer `'paragraphs'` will be collected from XML annotations and enveloped around EstNLTK's `'sentences'` layer; \n",
    "\n",
    "\n",
    "* **`discard_empty_paragraphs`** -- boolean specifying if empty paragraphs (paragraphs without textual content) should be discarded (default: True).\n",
    "\n",
    "\n",
    "\n",
    "* **`store_paragraph_attributes`** -- boolean specifying if attributes in the paragraph's XML tag will be collected and added as attributes of the corresponding layer (`'original_paragraphs'` or `'paragraphs'`) in a Text object. (default: False).\n",
    "\n",
    "\n",
    "* **`paragraph_separator`** -- string used as paragraph separator during the reconstruction of the text (default: `'\\n\\n'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing whole etTenTen with EstNLTK\n",
    "\n",
    "If you need to process the whole etTenTen with EstNLTK, you can use the command-line scripts in [ **`https://github.com/estnltk/estnltk-workflows`**](https://github.com/estnltk/estnltk-workflows). \n",
    "The workflow for processing etTenTen with EstNLTK, and saving the results as JSON format files is located at: \n",
    "[https://github.com/estnltk/estnltk-workflows/tree/master/estnltk_workflows/koondkorpus_and_ettenten_to_json](https://github.com/estnltk/estnltk-workflows/tree/master/estnltk_workflows/koondkorpus_and_ettenten_to_json). \n",
    "For details about processing, check out the `readme.md` file in the folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Importing texts of the Estonian National Corpus 2017 (_Eesti keele ühendkorpus 2017_)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
