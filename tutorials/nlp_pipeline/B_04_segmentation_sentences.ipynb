{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\"> B. Specific details for programmers: how it works</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:purple\"> Text segmentation: Sentences </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the layer of `words` is established, sentence boundaries between the words will be determined. In EstNLTK, the component responsible for this is the `SentenceTokenizer`.\n",
    "\n",
    "The `SentenceTokenizer` splits the text into sentences using NLTK's `PunktSentenceTokenizer` with an Estonian-specific model (available from [here](https://github.com/nltk/nltk_data/tree/gh-pages/packages/tokenizers)). After the initial sentence tokenization, `SentenceTokenizer` applies series of post-corrections to the obtained results. Post-corrections include removing sentence endings after non-ending abbreviations, adding sentence endings after emoticons, fixing endings in case of prolonged ending punctuation (e.g. `'...'`, or `'??'`), and merging together sentences that have been mistakenly split due to period-containing numeric expressions (e.g. dates/times), and periods nearby double quotes and parenthesis.\n",
    "\n",
    "In the following example, we create a text object, add the prerequisite layer (words), and then segment the text into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['Esimene', 'lõik', '.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Teine', 'lause', '.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Teine', 'lõik', '.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=0, end=7, text='Esimene'),\n",
       "Span(start=8, end=12, text='lõik'),\n",
       "Span(start=12, end=13, text='.')],\n",
       "ES[Span(start=14, end=19, text='Teine'),\n",
       "Span(start=20, end=25, text='lause'),\n",
       "Span(start=25, end=26, text='.')],\n",
       "ES[Span(start=28, end=33, text='Teine'),\n",
       "Span(start=34, end=38, text='lõik'),\n",
       "Span(start=38, end=39, text='.')]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from estnltk import Text\n",
    "from estnltk.taggers import SentenceTokenizer\n",
    "\n",
    "text = Text('''Esimene lõik. Teine lause.\n",
    "\n",
    "Teine lõik.''')\n",
    "text.tag_layer(['words'])\n",
    "SentenceTokenizer().tag(text)\n",
    "text['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-corrections of SentenceTokenizer\n",
    "\n",
    "After obtaining the initial sentence boundaries, the `SentenceTokenizer` applies post-corrections to fix erroneous sentence boundaries. Different types of fixes are grouped together and flags can be used to switch these fixes off (by default, all fixes are switched on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixes related to compound tokens (`fix_compound_tokens`)\n",
    "\n",
    "The list of sentence endings is filtered, and all the sentence endings that fall inside `compound_tokens` are removed. Special attention is paid on `compound_tokens` of type `non_ending_abbreviation`: a sentence ending added after `non_ending_abbreviation` will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['Lp.', 'esimees', ',', 'vt.', 'seda', 'joonist', 'seal', 'leheküljel', '.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=0, end=3, text='Lp.'),\n",
       "Span(start=4, end=11, text='esimees'),\n",
       "Span(start=11, end=12, text=','),\n",
       "Span(start=13, end=16, text='vt.'),\n",
       "Span(start=17, end=21, text='seda'),\n",
       "Span(start=22, end=29, text='joonist'),\n",
       "Span(start=30, end=34, text='seal'),\n",
       "Span(start=35, end=45, text='leheküljel'),\n",
       "Span(start=45, end=46, text='.')]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text('''Lp. esimees, vt. seda joonist seal leheküljel.''')\n",
    "text.tag_layer(['words'])\n",
    "SentenceTokenizer( fix_compound_tokens=True ).tag(text)  # fix sentence endings related to compound tokens (default)\n",
    "text['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `non_ending_abbreviation`-s, regular `abbreviations` in specific contexts can also cause wrong sentence endings. So, if `fix_compound_tokens=True`, then special merge patterns are also applied to detect regular abbreviations that are followed by period, and then by lowercase letters or non-ending punctuation (e.g. comma, semicolon). If a sentence break appears after the period in such contexts, then two consecutive sentences are joined together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['Jooniste', ',', 'tabelite', 'jm.', 'abil', 'on', 'kõik', 'selgeks', 'tehtud', '.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=0, end=8, text='Jooniste'),\n",
       "Span(start=8, end=9, text=','),\n",
       "Span(start=10, end=18, text='tabelite'),\n",
       "Span(start=19, end=22, text='jm.'),\n",
       "Span(start=23, end=27, text='abil'),\n",
       "Span(start=28, end=30, text='on'),\n",
       "Span(start=31, end=35, text='kõik'),\n",
       "Span(start=36, end=43, text='selgeks'),\n",
       "Span(start=44, end=50, text='tehtud'),\n",
       "Span(start=50, end=51, text='.')]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text('''Jooniste, tabelite jm. abil on kõik selgeks tehtud.''')\n",
    "\n",
    "text.tag_layer(['words'])\n",
    "SentenceTokenizer( fix_compound_tokens=True ).tag(text)  # switch on fixes related to compound tokens (default)\n",
    "text['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>compound_tokens</td>\n",
       "      <td>type, normalized</td>\n",
       "      <td>None</td>\n",
       "      <td>tokens</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['jm', '.']</td>\n",
       "      <td>('abbreviation',)</td>\n",
       "      <td>jm.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=compound_tokens, spans=SL[ES[Span(start=19, end=21, text='jm'),\n",
       "Span(start=21, end=22, text='.')]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['compound_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example: the compound token _'jm.'_ has type `abbreviation`, not `non_ending_abbreviation`, because it can also appear at the end of a sentence. So, we can only heuristically determine that it is likely not a sentence ending if the following text passage starts with lowercase letters or non-ending punctuation.\n",
    "\n",
    "Note: using this setting requires that `abbreviation`-s and `non_ending_abbreviation`-s are detected during the previous processing, and available in the layer `compound_tokens` (see `CompoundTokenTagger` for details);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixes related to numeric expressions (`fix_numeric`)\n",
    "\n",
    "Removes sentence endings that are mistakenly added after periods that end date, time and (other) numeric expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['17 .', 'okt', '.', '1998', 'a .', 'laekus', 'firmale', 'täpselt', '700.', '-', 'eeku', '.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=0, end=4, text='17 .'),\n",
       "Span(start=5, end=8, text='okt'),\n",
       "Span(start=8, end=9, text='.'),\n",
       "Span(start=10, end=14, text='1998'),\n",
       "Span(start=15, end=18, text='a .'),\n",
       "Span(start=19, end=25, text='laekus'),\n",
       "Span(start=26, end=33, text='firmale'),\n",
       "Span(start=34, end=41, text='täpselt'),\n",
       "Span(start=42, end=46, text='700.'),\n",
       "Span(start=46, end=47, text='-'),\n",
       "Span(start=48, end=52, text='eeku'),\n",
       "Span(start=52, end=53, text='.')]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text(\"17 . okt. 1998 a . laekus firmale täpselt 700.- eeku.\")\n",
    "\n",
    "text.tag_layer(['words'])\n",
    "SentenceTokenizer( fix_numeric=True ).tag(text)  # switch on fixes related to numeric expressions (default)\n",
    "text['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixes related to parentheses (`fix_parentheses`)\n",
    "\n",
    "Removes sentence endings that are mistakenly added inside parentheses, and fixes endings that are misplaced with respect to parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['(', 'Naerab', '.', ')']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Eriti', 'siis', ',', 'kui', 'sõidan', 'mootorratta', 'või', 'jalgrattaga', '(', 'v', '.', 'tasakaaluliikuriga', ')', '.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=0, end=1, text='('),\n",
       "Span(start=2, end=8, text='Naerab'),\n",
       "Span(start=8, end=9, text='.'),\n",
       "Span(start=10, end=11, text=')')],\n",
       "ES[Span(start=12, end=17, text='Eriti'),\n",
       "Span(start=18, end=22, text='siis'),\n",
       "Span(start=23, end=24, text=','),\n",
       "Span(start=25, end=28, text='kui'),\n",
       "Span(start=29, end=35, text='sõidan'),\n",
       "Span(start=36, end=47, text='mootorratta'),\n",
       "Span(start=48, end=51, text='või'),\n",
       "Span(start=52, end=63, text='jalgrattaga'),\n",
       "Span(start=64, end=65, text='('),\n",
       "Span(start=66, end=67, text='v'),\n",
       "Span(start=67, end=68, text='.'),\n",
       "Span(start=69, end=87, text='tasakaaluliikuriga'),\n",
       "Span(start=88, end=89, text=')'),\n",
       "Span(start=89, end=90, text='.')]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text(\"( Naerab. )\\nEriti siis , kui sõidan mootorratta või jalgrattaga ( v. tasakaaluliikuriga ).\")\n",
    "\n",
    "text.tag_layer(['words'])\n",
    "SentenceTokenizer( fix_parentheses=True ).tag(text)  # switch on fixes related to parentheses (default)\n",
    "text['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixes related to double quotes (`fix_double_quotes`)\n",
    "\n",
    "Removes sentence endings that are misplaced with respect to quotations / double quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['«', 'Meeste', 'lihas', 'on', 'tühjem', ',', 'aga', 'võtab', 'taastamistegevust', 'vastu', 'paremini', 'kui', 'varem', '.', '»']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['«', 'Meie', 'treeningutel', 'on', 'üks', 'uus', 'peateema', '!', '»', 'elavneb', 'Alaver', '.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=0, end=1, text='«'),\n",
       "Span(start=2, end=8, text='Meeste'),\n",
       "Span(start=9, end=14, text='lihas'),\n",
       "Span(start=15, end=17, text='on'),\n",
       "Span(start=18, end=24, text='tühjem'),\n",
       "Span(start=25, end=26, text=','),\n",
       "Span(start=27, end=30, text='aga'),\n",
       "Span(start=31, end=36, text='võtab'),\n",
       "Span(start=37, end=54, text='taastamistegevust'),\n",
       "Span(start=55, end=60, text='vastu'),\n",
       "Span(start=61, end=69, text='paremini'),\n",
       "Span(start=70, end=73, text='kui'),\n",
       "Span(start=74, end=79, text='varem'),\n",
       "Span(start=80, end=81, text='.'),\n",
       "Span(start=82, end=83, text='»')],\n",
       "ES[Span(start=84, end=85, text='«'),\n",
       "Span(start=86, end=90, text='Meie'),\n",
       "Span(start=91, end=103, text='treeningutel'),\n",
       "Span(start=104, end=106, text='on'),\n",
       "Span(start=107, end=110, text='üks'),\n",
       "Span(start=111, end=114, text='uus'),\n",
       "Span(start=115, end=123, text='peateema'),\n",
       "Span(start=124, end=125, text='!'),\n",
       "Span(start=126, end=127, text='»'),\n",
       "Span(start=128, end=135, text='elavneb'),\n",
       "Span(start=136, end=142, text='Alaver'),\n",
       "Span(start=143, end=144, text='.')]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text(\"\"\"« Meeste lihas on tühjem , aga võtab taastamistegevust vastu paremini kui varem . »\n",
    "« Meie treeningutel on üks uus peateema ! » elavneb Alaver .\"\"\")\n",
    "\n",
    "text.tag_layer(['words'])\n",
    "SentenceTokenizer( fix_double_quotes=True ).tag(text)  # switch on fixes related to double quotes (default)\n",
    "text['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixes related to inner titles ending with punctuation (`fix_inner_title_punct`)\n",
    "\n",
    "Removes sentence endings that are mistakenly placed after titles inside the sentence. Currently only fixes cases when a question mark or an exclamation mark is followed by a sentence ending and, a colon or a semicolon starts the next sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['Laval', 'olid', 'jõulise', 'naissolistiga', 'Conflict', 'OK', '!', ',', 'kitarripoppi', 'mängivad', 'Claires', 'Birthday', 'ja', 'Seachers', '.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=0, end=5, text='Laval'),\n",
       "Span(start=6, end=10, text='olid'),\n",
       "Span(start=11, end=18, text='jõulise'),\n",
       "Span(start=19, end=32, text='naissolistiga'),\n",
       "Span(start=33, end=41, text='Conflict'),\n",
       "Span(start=42, end=44, text='OK'),\n",
       "Span(start=44, end=45, text='!'),\n",
       "Span(start=45, end=46, text=','),\n",
       "Span(start=47, end=59, text='kitarripoppi'),\n",
       "Span(start=60, end=68, text='mängivad'),\n",
       "Span(start=69, end=76, text='Claires'),\n",
       "Span(start=77, end=85, text='Birthday'),\n",
       "Span(start=86, end=88, text='ja'),\n",
       "Span(start=89, end=97, text='Seachers'),\n",
       "Span(start=97, end=98, text='.')]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text(\"Laval olid jõulise naissolistiga Conflict OK!, kitarripoppi mängivad Claires Birthday ja Seachers.\")\n",
    "\n",
    "text.tag_layer(['words'])\n",
    "SentenceTokenizer( fix_inner_title_punct=True ).tag(text)  # switch on fixes related to inner titles (default)\n",
    "text['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixes related to prolonged sentence ending punctuation (`fix_repeated_ending_punct`)\n",
    "\n",
    "Adds sentence endings that are missed in places of prolonged ending punctuation (including ellipsis / triple dots), and also  fixes misplaced sentence endings in such contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['Seda', 'ma', 'ei', 'teadnud', '...']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Ja', 'tegelikult', 'ei', 'saanudki', 'teada', '!', '!!']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=0, end=4, text='Seda'),\n",
       "Span(start=5, end=7, text='ma'),\n",
       "Span(start=8, end=10, text='ei'),\n",
       "Span(start=11, end=18, text='teadnud'),\n",
       "Span(start=18, end=21, text='...')],\n",
       "ES[Span(start=22, end=24, text='Ja'),\n",
       "Span(start=25, end=35, text='tegelikult'),\n",
       "Span(start=36, end=38, text='ei'),\n",
       "Span(start=39, end=47, text='saanudki'),\n",
       "Span(start=48, end=53, text='teada'),\n",
       "Span(start=54, end=55, text='!'),\n",
       "Span(start=56, end=58, text='!!')]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text(\"Seda ma ei teadnud... Ja tegelikult ei saanudki teada ! !! \")\n",
    "\n",
    "text.tag_layer(['words'])\n",
    "SentenceTokenizer( fix_repeated_ending_punct=True ).tag(text)  # switch on fixes related to repeated ending punct (default)\n",
    "text['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use emoticons as sentence endings (`use_emoticons_as_endings`)\n",
    "\n",
    "If switched on (the default setting), then emoticons are treated as sentence endings. Note: this requires that emoticons are detected during the previous processing, and available in the layer `compound_tokens` (see `CompoundTokenTagger` for details);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['Nii', 'habras', ',', 'ilus', 'ja', 'minu', 'oma', ':)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Kõige', 'parem', 'mis', 'kunagi', 'juhtuda', 'saab', ':)', ':)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Magamata', 'öid', 'mul', 'muidugi', 'ei', 'olnud', '.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=0, end=3, text='Nii'),\n",
       "Span(start=4, end=10, text='habras'),\n",
       "Span(start=10, end=11, text=','),\n",
       "Span(start=12, end=16, text='ilus'),\n",
       "Span(start=17, end=19, text='ja'),\n",
       "Span(start=20, end=24, text='minu'),\n",
       "Span(start=25, end=28, text='oma'),\n",
       "Span(start=29, end=31, text=':)')],\n",
       "ES[Span(start=32, end=37, text='Kõige'),\n",
       "Span(start=38, end=43, text='parem'),\n",
       "Span(start=44, end=47, text='mis'),\n",
       "Span(start=48, end=54, text='kunagi'),\n",
       "Span(start=55, end=62, text='juhtuda'),\n",
       "Span(start=63, end=67, text='saab'),\n",
       "Span(start=68, end=70, text=':)'),\n",
       "Span(start=70, end=72, text=':)')],\n",
       "ES[Span(start=73, end=81, text='Magamata'),\n",
       "Span(start=82, end=85, text='öid'),\n",
       "Span(start=86, end=89, text='mul'),\n",
       "Span(start=90, end=97, text='muidugi'),\n",
       "Span(start=98, end=100, text='ei'),\n",
       "Span(start=101, end=106, text='olnud'),\n",
       "Span(start=106, end=107, text='.')]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text(\"Nii habras, ilus ja minu oma :) Kõige parem mis kunagi juhtuda saab :):) Magamata öid mul muidugi ei olnud.\")\n",
    "\n",
    "text.tag_layer(['words'])\n",
    "SentenceTokenizer( use_emoticons_as_endings=True ).tag(text)  # switch on using emoticons as sentence endings (default)\n",
    "text['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix paragraph endings (`fix_paragraph_endings`)\n",
    "\n",
    "If switched on (the default setting), then paragraph endings (double newlines) are treated as sentence endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['Herbes', 'de', 'Provence', 'maitseainesegu']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Teistes', 'keeltes']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['English', ':', 'herbes', 'de', 'Provence', ',', 'Provençal', 'herbs']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['French', ':', 'herbes', 'de', 'Provence']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Kirjeldus']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['1970ndatel', 'prantsuse', 'köögis', 'populaarseks', 'muutunud', 'maitseainesegu', '.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=1, end=7, text='Herbes'),\n",
       "Span(start=8, end=10, text='de'),\n",
       "Span(start=11, end=19, text='Provence'),\n",
       "Span(start=20, end=34, text='maitseainesegu')],\n",
       "ES[Span(start=36, end=43, text='Teistes'),\n",
       "Span(start=44, end=51, text='keeltes')],\n",
       "ES[Span(start=53, end=60, text='English'),\n",
       "Span(start=60, end=61, text=':'),\n",
       "Span(start=62, end=68, text='herbes'),\n",
       "Span(start=69, end=71, text='de'),\n",
       "Span(start=72, end=80, text='Provence'),\n",
       "Span(start=80, end=81, text=','),\n",
       "Span(start=82, end=91, text='Provençal'),\n",
       "Span(start=92, end=97, text='herbs')],\n",
       "ES[Span(start=99, end=105, text='French'),\n",
       "Span(start=105, end=106, text=':'),\n",
       "Span(start=107, end=113, text='herbes'),\n",
       "Span(start=114, end=116, text='de'),\n",
       "Span(start=117, end=125, text='Provence')],\n",
       "ES[Span(start=127, end=136, text='Kirjeldus')],\n",
       "ES[Span(start=138, end=148, text='1970ndatel'),\n",
       "Span(start=149, end=158, text='prantsuse'),\n",
       "Span(start=159, end=165, text='köögis'),\n",
       "Span(start=166, end=178, text='populaarseks'),\n",
       "Span(start=179, end=187, text='muutunud'),\n",
       "Span(start=188, end=202, text='maitseainesegu'),\n",
       "Span(start=202, end=203, text='.')]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text('''\n",
    "Herbes de Provence maitseainesegu\n",
    "\n",
    "Teistes keeltes\n",
    "\n",
    "English: herbes de Provence, Provençal herbs\n",
    "\n",
    "French: herbes de Provence\n",
    "\n",
    "Kirjeldus\n",
    "\n",
    "1970ndatel prantsuse köögis populaarseks muutunud maitseainesegu.\n",
    "''')\n",
    "\n",
    "text.tag_layer(['words'])\n",
    "SentenceTokenizer( fix_paragraph_endings=True ).tag(text)  # switch on using double newlines as sentence endings (default)\n",
    "text['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: `fix_paragraph_endings` has higher priority than fixes made by merge rules (see the step \"_applying merge patterns (and merge-and-split patterns)_\" in technical details below). This means that if `fix_paragraph_endings` has been switched on, and a paragraph ending is between two sentences that could be merged by the rules, then the merging will be cancelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical details\n",
    "\n",
    "The initial sentence tokenization is obtained via `PunktSentenceTokenizer`'s method [sentences_from_tokens()]( http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_tokens), which takes a  list of words as an input, and groups the words into sentences. Using this method ensures that:\n",
    "\n",
    "  * words consisting of compound tokens (e.g. names with initials like _'A. H. Tammsaare'_) will be treated as single text units, and sentence boundaries will not be added mistakenly inside such words;\n",
    "  * if a sentence ending symbol mistakenly \"glues together\" words in text (e.g. `'Kas on niipalju vaja?Ei ole ju.'`), then the list of words maintains the correct separation (e.g. `['Kas', 'on', 'niipalju', 'vaja', '?', 'Ei', 'ole', 'ju', '.']`), and provides basis for a correct sentence tokenization (otherwise, the sentence boundary would be missed because of the \"words glued together\");\n",
    "\n",
    "  _Remark_: If you really need to, then you can also change the initial sentence tokenizer (see the subsection _\"Customizing base tokenizer of the `SentenceTokenizer`\"_ below), but please keep in mind that post-corrections of `SentenceTokenizer` have been specifically created for the `PunktSentenceTokenizer`, and they may not work properly with other sentence tokenizers;\n",
    "\n",
    "After the initial sentence tokenization, the following post-correction steps are applied:\n",
    "\n",
    "  1. _fixing compound tokens_ ( flag `fix_compound_tokens` ) -- a built-in logic is used to remove all sentence endings that fall inside `compound_tokens`, and also sentence endings that are added after `compound_tokens` of type `non_ending_abbreviation` are removed. These fixes also have a continuation, see 6.1 for details;\n",
    "  2. _fixing repeated ending punctuation_ ( flag `fix_repeated_ending_punct` ) -- a built-in logic is used to add sentence endings after prolonged ending punctuation (including ellipsis/triple dots) if the prolonged ending punctuation is followed by a titlecased word. An exception: if prolonged ending punctuation is immediately after starting brackets, then sentence ending won't be added. These fixes also have a continuation, see 6.2 for details;\n",
    "  3. _adding sentence endings after emoticons_ ( flag `use_emoticons_as_endings` ) -- a built-in logic is used to add sentence endings after emoticons (`compound_tokens` of type `emoticon`) if emoticons are followed by a titlecased word. Also, if a sentence ending already exists before the first emoticon, then it is removed (to assure that emoticons belong with the ending sentence);\n",
    "  4. _collecting spans of potential sentences_ -- a built-in logic is used to collect spans (starts, ends) of potential sentences. Note that in steps 1-3, only sentence endings were processed, and at this step, full sentence spans are created. As a side effect, spans are also aligned with starts and ends of the words;\n",
    "  5. _fixing sentence endings related to paragraph endings_ ( flag `fix_paragraph_endings` ) -- a built-in logic is used to add sentence breaks in places where paragraphs end. The current logic marks double newlines as paragraph endings;\n",
    "  6. _applying merge patterns (and merge-and-split patterns)_ --- merging patterns are applied to join together consecutive \"sentences\" if the sentence break between them was erroneous. Patterns include a special subset called _merge-and-split patterns_ which first join two sentences together, and then split into two sentences at some other location inside one of the sentences. Merging patterns are divided into different types, which can be switched off / on by passing flags to `SentenceTokenizer`'s constructor. The following types of patterns are applied by default:\n",
    "\n",
    "  6.1. _fixing sentence endings mistakenly added after regular `abbreviation`-s_ ( flag `fix_compound_tokens` ) -- if a regular `abbreviation` is followed by a sentence break, and then by a lowercase word or non-ending punctuation (comma or semicolon), then the sentence break is removed after such abbreviation. Patterns that are applied in this step have `'fix_type'` that starts with the prefix `'abbrev'`;   \n",
    "  6.2. _fixing sentence endings mistakenly added inside prolonged punctuation_ ( flag `fix_repeated_ending_punct` ) -- if there is a sentence break inside the prolonged ending punctuation (e.g. the last exclamation mark forms a new sentence), then the sentence break will be removed. Merge patterns that are applied in this step have `'fix_type'` that starts with the prefix `'repeated_ending_punct'`;  \n",
    "  6.3. _removing sentence endings that were mistakenly added after periods that end date, time and (other) numeric expressions_ ( flag `fix_numeric`). Merge patterns applied in this step have `'fix_type'` starting with `'numeric'`;  \n",
    "  6.4. _fixing sentence endings that were misplaced with respect to parentheses_ ( flag `fix_parentheses` ). Merge patterns applied in this step have `'fix_type'` starting with `'parentheses'`;  \n",
    "  6.5. _fixing sentence endings that were misplaced with respect to quotations / double quotes_ ( flag `fix_double_quotes` ); Merge patterns applied in this step have `'fix_type'` starting with `'double_quotes'`;    \n",
    "  6.6. _removing sentence endings that were mistakenly placed after titles inside the sentence_ ( flag `fix_inner_title_punct` ); Merge patterns applied in this step have `'fix_type'` starting with `'inner_title_punct'`;       \n",
    "\n",
    "The final step of `SentenceTokenizer` is creating the layer 'sentences' based on the fixed / post-corrected list of sentence spans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `record_fix_types=True` is passed as a parameter to the `SentenceTokenizer`'s constructor, then the layer `'sentences'` will have attribute `'fix_types'` containing information about which types of merge patterns (from the step 6) were applied on the sentences. This information can be used for testing / debugging purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge patterns (and merge-and-split patterns)\n",
    "\n",
    "Each merge pattern contains two regular expressions describing two consecutive sentences that need to be joined. The list of patterns is defined in the variable `merge_patterns` inside the module `estnltk.taggers.text_segmentation.sentence_tokenizer`. The following is an example of a pattern that merges sentences that have been mistakenly broken in the middle of a range of ordinal numbers:\n",
    "\n",
    "      { 'comment'  : '{Numeric_range_start} {period} + {dash} {Numeric_range_end}', \\\n",
    "        'example'  : '\"Tartu Muinsuskaitsepäevad toimusid 1988. a 14.\" + \"- 17. aprillil.\"', \\\n",
    "        'fix_type' : 'numeric_range', \\\n",
    "        'regexes'  : [ re.compile('(.+)?([0-9]+)\\s*\\.$', re.DOTALL), \\\n",
    "                   re.compile('-+\\s*([0-9]+)\\s*\\.(.*)?$', re.DOTALL)], \\\n",
    "      },\n",
    "      \n",
    "Attribute `'comment'` is used to give a generic description of the pattern, and `'example'` exemplifies the string joining performed by the pattern. Although these attributes are not mandatory, it is highly advisable to use them when adding new entries, as it helps to maintain interpretability.\n",
    "\n",
    "Attribute `'fix_type'` is mandatory and expresses the type of the fix. Flags passed to `SentenceTokenizer`'s constructor instruct which types of fixes will be used during the post-correction, and which ones will be skipped. For instance, the previously exemplified pattern will only be used if the flag `fix_numeric` is switched on. See 'Technical details' above for more information.\n",
    "\n",
    "Attribute `'regexes'` should be a list containing exactly two precompiled regular expressions that are used for finding the joining spot. The first should describe a sentence ending, and the second a beginning of a follow-up sentence.\n",
    "\n",
    "Attribute `'shift_end'` is an optional boolean attribute, which can be used to turn the pattern into a _merge-and-split pattern_. If switched on, then one of the regular expressions defined in `'regexes'` should contain a group named `'end'` (see \n",
    "[how to define named groups](https://docs.python.org/3.5/howto/regex.html#non-capturing-and-named-groups)), which marks the new sentence end. After two sentences are joined together, a new sentence end is created at the end of the string captured by the group `'end'`. Note that if the group  `'end'` is not defined, or it does not match, then sentences are only merged with no following split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Improving  `SentenceTokenizer`'s rules: an example\n",
    "\n",
    "If you are analysing texts from a specific domain, you may encounter situations where you need to improve `SentenceTokenizer`'s existing rules, or add additional rules. Here is a short example how to do it.\n",
    "\n",
    "Let's consider an example text consisting of two sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = Text('Meie puuvarud: 3 rm. märgasid lepahalge sellest aastast, 4 rm. poolkuivasid '+\\\n",
    "            'halge eelmisest aastast ning kuivi halge 2 rm. Kas sellest piisab?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the sentence segmentation, the first sentence is mistakenly split into three sentences because of misinterpreting the period after the abbreviation _rm_ as sentence ending:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['Meie', 'puuvarud', ':', '3', 'rm', '.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['märgasid', 'lepahalge', 'sellest', 'aastast', ',', '4', 'rm', '.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['poolkuivasid', 'halge', 'eelmisest', 'aastast', 'ning', 'kuivi', 'halge', '2', 'rm', '.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Kas', 'sellest', 'piisab', '?']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=0, end=4, text='Meie'),\n",
       "Span(start=5, end=13, text='puuvarud'),\n",
       "Span(start=13, end=14, text=':'),\n",
       "Span(start=15, end=16, text='3'),\n",
       "Span(start=17, end=19, text='rm'),\n",
       "Span(start=19, end=20, text='.')],\n",
       "ES[Span(start=21, end=29, text='märgasid'),\n",
       "Span(start=30, end=39, text='lepahalge'),\n",
       "Span(start=40, end=47, text='sellest'),\n",
       "Span(start=48, end=55, text='aastast'),\n",
       "Span(start=55, end=56, text=','),\n",
       "Span(start=57, end=58, text='4'),\n",
       "Span(start=59, end=61, text='rm'),\n",
       "Span(start=61, end=62, text='.')],\n",
       "ES[Span(start=63, end=75, text='poolkuivasid'),\n",
       "Span(start=76, end=81, text='halge'),\n",
       "Span(start=82, end=91, text='eelmisest'),\n",
       "Span(start=92, end=99, text='aastast'),\n",
       "Span(start=100, end=104, text='ning'),\n",
       "Span(start=105, end=110, text='kuivi'),\n",
       "Span(start=111, end=116, text='halge'),\n",
       "Span(start=117, end=118, text='2'),\n",
       "Span(start=119, end=121, text='rm'),\n",
       "Span(start=121, end=122, text='.')],\n",
       "ES[Span(start=123, end=126, text='Kas'),\n",
       "Span(start=127, end=134, text='sellest'),\n",
       "Span(start=135, end=141, text='piisab'),\n",
       "Span(start=141, end=142, text='?')]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.tag_layer(['sentences'])\n",
    "text.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tricky case: if we look at the sentence, we can see that the abbreviation _rm_ can be in the middle of the sentence, but it can also end the sentence. \n",
    "So, while we can solve part of the problem by adding _rm_ to the list of `non_ending_abbreviation`-s in `CompoundTokenTagger` (see the tutorial `B_02_*` for the details), this would still yield us one error: the last _rm_ would then be mistakenly considered as being in the middle of the sentence, and we will end up having one big sentence (instead of two).\n",
    "\n",
    "The workaround here is to introduce a new merge pattern, which will cancel the sentence break after the string `'rm.'` if the string is followed by an unlikely sentence start (e.g. lowercase letter).\n",
    "\n",
    "The module `estnltk.taggers.text_segmentation.sentence_tokenizer` has the list of `merge_patterns`, which contains all merge patterns (and merge-and-split patterns). Let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from estnltk.taggers.text_segmentation.sentence_tokenizer import merge_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create our own pattern (see the description of the format in the subsection _Merge patterns_ above), and add it to the `merge_patterns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "# Create a new post-correction\n",
    "rm_fix = \\\n",
    "{ 'comment'  : '{rm} {period} + {lowercase letter}', \\\n",
    "  'example'  : '\"Meie puuvarud: 3 rm.\" + \"märgasid lepahalge\"', \\\n",
    "  'fix_type' : 'abbrev_common', \\\n",
    "  'regexes'  : [ re.compile('(.+)?\\srm\\s*\\.$', re.DOTALL), \\\n",
    "                 re.compile('^([a-zöäüõžš])\\s*(.*)?$', re.DOTALL)], \\\n",
    "}\n",
    "# Add it to the list of corrections\n",
    "merge_patterns.append( rm_fix )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now, we have updated the default set of rules. Next, we must reimport the `SentenceTokenizer`, so that it will reload the rules, and use the new rules instead of the old ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-import SentenceTokenizer:\n",
    "# 1) Remove the old SentenceTokenizer\n",
    "del SentenceTokenizer\n",
    "# 2) Import the new one\n",
    "from estnltk.taggers.text_segmentation.sentence_tokenizer import SentenceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Note: removing old `SentenceTokenizer` is only required when you have previously imported the class. Otherwise, it should be enough if you only import `SentenceTokenizer`.\n",
    "  \n",
    "Finally, let's use our improved tagger to analyse the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['Meie', 'puuvarud', ':', '3', 'rm', '.', 'märgasid', 'lepahalge', 'sellest', 'aastast', ',', '4', 'rm', '.', 'poolkuivasid', 'halge', 'eelmisest', 'aastast', 'ning', 'kuivi', 'halge', '2', 'rm', '.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Kas', 'sellest', 'piisab', '?']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=0, end=4, text='Meie'),\n",
       "Span(start=5, end=13, text='puuvarud'),\n",
       "Span(start=13, end=14, text=':'),\n",
       "Span(start=15, end=16, text='3'),\n",
       "Span(start=17, end=19, text='rm'),\n",
       "Span(start=19, end=20, text='.'),\n",
       "Span(start=21, end=29, text='märgasid'),\n",
       "Span(start=30, end=39, text='lepahalge'),\n",
       "Span(start=40, end=47, text='sellest'),\n",
       "Span(start=48, end=55, text='aastast'),\n",
       "Span(start=55, end=56, text=','),\n",
       "Span(start=57, end=58, text='4'),\n",
       "Span(start=59, end=61, text='rm'),\n",
       "Span(start=61, end=62, text='.'),\n",
       "Span(start=63, end=75, text='poolkuivasid'),\n",
       "Span(start=76, end=81, text='halge'),\n",
       "Span(start=82, end=91, text='eelmisest'),\n",
       "Span(start=92, end=99, text='aastast'),\n",
       "Span(start=100, end=104, text='ning'),\n",
       "Span(start=105, end=110, text='kuivi'),\n",
       "Span(start=111, end=116, text='halge'),\n",
       "Span(start=117, end=118, text='2'),\n",
       "Span(start=119, end=121, text='rm'),\n",
       "Span(start=121, end=122, text='.')],\n",
       "ES[Span(start=123, end=126, text='Kas'),\n",
       "Span(start=127, end=134, text='sellest'),\n",
       "Span(start=135, end=141, text='piisab'),\n",
       "Span(start=141, end=142, text='?')]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create text and tag prerequisite layers:\n",
    "text = Text('Meie puuvarud: 3 rm. märgasid lepahalge sellest aastast, 4 rm. poolkuivasid '+\\\n",
    "            'halge eelmisest aastast ning kuivi halge 2 rm. Kas sellest piisab?')\n",
    "text.tag_layer(['words'])\n",
    "# Tag sentences with the new post-corrections\n",
    "SentenceTokenizer().tag(text)\n",
    "text.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Mission accomplished!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customizing base tokenizer of the `SentenceTokenizer`\n",
    "\n",
    "If you really need to, then you can also customize the `SentenceTokenizer`, and change it's base tokenizer from `PunktSentenceTokenizer` to some other tokenizer that inherits from [`nltk.tokenize.api.TokenizerI`](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI). For example, if you have an input text, where each sentence is systematically placed on a new line, then you may want to use NLTK's [`LineTokenizer`](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.simple.LineTokenizer) instead of the default `PunktSentenceTokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a sentence tokenizer that only splits sentences in places of new lines\n",
    "from nltk.tokenize.simple import LineTokenizer\n",
    "newline_sentence_tokenizer = SentenceTokenizer( base_sentence_tokenizer=LineTokenizer() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['See', 'on', 'esimene', 'lause']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Ja', 'see', 'teine', 'lause']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['Kolmas', 'lause', 'on', 'kolmandal', 'real']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name=sentences, spans=SL[ES[Span(start=1, end=4, text='See'),\n",
       "Span(start=5, end=7, text='on'),\n",
       "Span(start=8, end=15, text='esimene'),\n",
       "Span(start=16, end=21, text='lause')],\n",
       "ES[Span(start=22, end=24, text='Ja'),\n",
       "Span(start=25, end=28, text='see'),\n",
       "Span(start=29, end=34, text='teine'),\n",
       "Span(start=35, end=40, text='lause')],\n",
       "ES[Span(start=41, end=47, text='Kolmas'),\n",
       "Span(start=48, end=53, text='lause'),\n",
       "Span(start=54, end=56, text='on'),\n",
       "Span(start=57, end=66, text='kolmandal'),\n",
       "Span(start=67, end=71, text='real')]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare text\n",
    "text = Text('''\n",
    "See on esimene lause\n",
    "Ja see teine lause\n",
    "Kolmas lause on kolmandal real\n",
    "''')\n",
    "text.tag_layer(['words'])\n",
    "# Apply the customized sentence tokenizer\n",
    "newline_sentence_tokenizer.tag(text)\n",
    "text['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Things to keep in mind:_\n",
    " * by default, the method [sentences_from_tokens()]( http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_tokens) is used for tokenization, but if the new `base_sentence_tokenizer` does not have that method, then the method [`span_tokenize()`](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI.span_tokenize) is used instead;\n",
    " * the post-corrections have been created specifically for the default `base_sentence_tokenizer` (which is: `PunktSentenceTokenizer` with the Estonian-specific model). If you change the `base_sentence_tokenizer`, then there is no guarantee that all the post-corrections still work properly. So, it may be a good idea to turn off the post-corrections while using a custom `base_sentence_tokenizer`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Known limitations and points for further improvement\n",
    "\n",
    " * `SentenceTokenizer` provides sentence ending fixes related to the most commonly used abbreviations and acronyms (that were found during the analysis of a sample from [KoondKorpus](https://keeleressursid.ee/et/keeleressursid-cl-ut/korpused/83-article/clutee-lehed/192-segakorpus) and [etTenTen](http://www2.keeleveeb.ee/dict/corpus/ettenten/about.html)). If you need to analyse a corpus from a specific domain, you likely need to provide additional / domain-specific fixes. For this purpose, `merge_patterns` defined in `estnltk.taggers.text_segmentation.sentence_tokenizer` could be augmented with additional rules;\n",
    " * Sentence breaks can be erroneously added inside _enumerations that contain periods_ (such as `1. ... ; 2. ... ; 3. ...`, or `a. ... ; b. ... ; c. ...`). Note that resloving these cases requires that enumerations are first detected in the text (so that enumerations are made distinct from sentences ending with numbers). As one enumeration item can contain several sentences, this problem goes beyond checking tokens in close proximity (as done in `CompoundTokenTagger`), and checking ends and starts of consecutive sentences (as done in `SentenceTokenizer`) -- a special logic involving analysis of whole document is likely required for detection of enumerations;\n",
    " * A sentence break can be erroneously added after a regular `abbreviation` followed by a titlecased word. For instance, in the sentence `'Bodhidharma tõi zeni 6. sajandil e.Kr. Hiinasse.'`, the break is added after `'e.Kr.'`, although the actual sentence continues. A solution to this problem would involve verifying that the titlecased word is actually a proper noun (or a named entity), and also checking that the second sentence is not too small (or that it contains verbs);"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
